{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygrib\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gribName=\"20151018_1024_9b49a5d5\"\n",
    "cyc_name=str.upper(\"Patricia\")\n",
    "grbs = pygrib.open('.\\\\Data\\\\'+gribName+'.grib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grb=grbs[1:]\n",
    "grb.append(grbs[len(grb)+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_grbs=grbs.select(shortName=\"u\",typeOfLevel='isobaricInhPa',level=[500,700,800,925,1000])\n",
    "#selected_grbs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numChannels=12\n",
    "gridShape=(221,381)\n",
    "framesPerDay=24\n",
    "isoLvls=[500,700,850,925,1000]\n",
    "numIsoLvls=len(isoLvls)\n",
    "numFrames=len(grb)\n",
    "numDays=numFrames//(numChannels*framesPerDay*numIsoLvls)\n",
    "#INDEX=Frame*5*12+IsoLvl*12+Channel (tot=12*5*24*numDays)\n",
    "assert numFrames%(numChannels*framesPerDay*numIsoLvls)==0 #Having integer number of days\n",
    "assert grb[1].values.shape==gridShape #Check grid shape matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firstFrame=grb[0]['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grb[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grb[0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 5, 12, 221, 381)\n"
     ]
    }
   ],
   "source": [
    "arr=np.zeros(shape=(numDays*framesPerDay,numIsoLvls,numChannels,*gridShape),dtype=\"float32\")\n",
    "print(arr.shape)\n",
    "\n",
    "def parseIndices(i):\n",
    "    timeLoc=i//(numChannels*numIsoLvls)\n",
    "    isoLoc=(i-timeLoc*(numChannels*numIsoLvls))//numChannels\n",
    "    channelLoc=i-timeLoc*(numChannels*numIsoLvls)-isoLoc*numChannels\n",
    "    return timeLoc,isoLoc,channelLoc\n",
    "\n",
    "#INDEX=Frame*60+IsoLvl*12+Channel (tot=12*5*24*numDays)\n",
    "for i,v in enumerate(grb):\n",
    "    arr[*parseIndices(i)] = v.values.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parseIndices(247)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grb[247].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del grb\n",
    "del grbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr[0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_ref_minmax(data):\n",
    "    assert data.shape[2]==60 #60 channels\n",
    "    minarr=np.zeros(60)\n",
    "    maxarr=np.zeros(60)\n",
    "    for channel in range(data.shape[2]):  # Assuming the third index is for channels\n",
    "        # Extract the channel data\n",
    "        channel_data = data[:, :, channel, :, :]\n",
    "        \n",
    "        # Compute the min and max\n",
    "        minarr[channel] = np.min(channel_data)\n",
    "        maxarr[channel] = np.max(channel_data)\n",
    "    return minarr,maxarr\n",
    "\n",
    "# Assuming `arr` is your ndarray with the shape (numDays*framesPerDay, numIsoLvls, numChannels, *gridShape)\n",
    "def normalize_channels(data,minarr,maxarr):\n",
    "    # data.shape should be (numDays*framesPerDay, numIsoLvls, numChannels, height, width)\n",
    "    assert data.shape[2]==60 #60 channels\n",
    "    # Initialize a new array to hold the normalized data\n",
    "    normalized_data = np.zeros_like(data, dtype=np.float32)\n",
    "    \n",
    "    # Iterate over each channel to normalize\n",
    "    for channel in range(data.shape[2]):  # Assuming the third index is for channels\n",
    "        # Extract the channel data\n",
    "        channel_data = data[:, :, channel, :, :]\n",
    "        \n",
    "        # OBTAIN the min and max\n",
    "        min_val = minarr[channel]\n",
    "        max_val = maxarr[channel]\n",
    "        \n",
    "        # Avoid division by zero in case all values in the channel are the same\n",
    "        if max_val > min_val:\n",
    "            # Normalize the channel\n",
    "            normalized_data[:, :, channel, :, :] = (channel_data - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            # If max equals min, set the normalized data to zero or handle appropriately\n",
    "            normalized_data[:, :, channel, :, :] = 0  # Or set it to a neutral value like 0.5 if more appropriate\n",
    "    return normalized_data\n",
    "\n",
    "minref,maxref=find_ref_minmax(arr)\n",
    "# Apply the normalization to your data\n",
    "normalized_arr = normalize_channels(arr,minref,maxref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming normalized_arr is your ndarray\n",
    "# Example shape of normalized_arr is (numDays*framesPerDay, numIsoLvls, numChannels, height, width)\n",
    "# We need to transform it to (numDays*framesPerDay, height, width, numChannels*numIsoLvls)\n",
    "\n",
    "# First, let's transpose the axes to bring all spatial dimensions next to each other\n",
    "# Transpose (0, 3, 4, 1, 2) means:\n",
    "# 0 -> 0 (keep numDays*framesPerDay at the first axis)\n",
    "# 1 -> 3 (move the first spatial dimension to second place)\n",
    "# 2 -> 4 (move the second spatial dimension to third place)\n",
    "# 3 -> 1 (move numIsoLvls to the last but one)\n",
    "# 4 -> 2 (move numChannels to the last position)\n",
    "transposed_arr = np.transpose(normalized_arr, (0, 3, 4, 1, 2))\n",
    "\n",
    "# Now, we need to merge the last two dimensions (numIsoLvls and numChannels)\n",
    "# Calculate the new dimension\n",
    "new_last_dim = transposed_arr.shape[3] * transposed_arr.shape[4]\n",
    "\n",
    "# Reshape to merge the last two dimensions\n",
    "inTensor = transposed_arr.reshape(transposed_arr.shape[0], transposed_arr.shape[1], transposed_arr.shape[2], new_last_dim)\n",
    "\n",
    "# reshaped_arr now has the shape (numDays*framesPerDay, height, width, numChannels*numIsoLvls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# # Assuming each \"frame\" is an independent input not wrapped in TimeDistributed\n",
    "# input_layer = Input(shape=(13, *gridShape, 60))  # Adjust based on your exact gridShape and channel info\n",
    "\n",
    "# # CNN layers applied independently to each frame (you might need a loop or custom layer here)\n",
    "# # Simplified version: Applying CNN to the first frame for illustration (adjust this part!)\n",
    "# cnn_out = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_layer[:,0,:,:,:])\n",
    "# cnn_out = MaxPooling2D(pool_size=(3, 3))(cnn_out)\n",
    "# cnn_out = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(cnn_out)\n",
    "# cnn_out = MaxPooling2D(pool_size=(3, 3))(cnn_out)\n",
    "# cnn_out = Conv2D(filters=128, kernel_size=(3, 3), activation='relu')(cnn_out)\n",
    "# cnn_out = MaxPooling2D(pool_size=(2, 2))(cnn_out)\n",
    "# cnn_out = Flatten()(cnn_out)\n",
    "\n",
    "# # Scalar inputs for the best track data at present (shape=(6,))\n",
    "# best_track_input = Input(shape=(6,))\n",
    "\n",
    "# # Concatenate the scalar values with the output of the CNN\n",
    "# combined_inputs = Concatenate(axis=-1)([cnn_out, best_track_input])\n",
    "\n",
    "# # Dense layers for final prediction\n",
    "# dense_out = Dense(units=100, activation='relu')(combined_inputs)\n",
    "# output = Dense(units=4)(dense_out)  # Predicting values for the cyclone's next location\n",
    "\n",
    "# # Compile the model\n",
    "# model = Model(inputs=[input_layer, best_track_input], outputs=output)\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gridShape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Input shape considering each frame is now part of a sequence\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[43mgridShape\u001b[49m, \u001b[38;5;241m60\u001b[39m))  \u001b[38;5;66;03m# Adjust the gridShape and channels as necessary\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Apply CNN to each frame independently using TimeDistributed\u001b[39;00m\n\u001b[0;32m      8\u001b[0m cnn_out \u001b[38;5;241m=\u001b[39m TimeDistributed(Conv2D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))(input_layer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gridShape' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Input shape considering each frame is now part of a sequence\n",
    "input_layer = Input(shape=(13, *gridShape, 60))  # Adjust the gridShape and channels as necessary\n",
    "\n",
    "# Apply CNN to each frame independently using TimeDistributed\n",
    "cnn_out = TimeDistributed(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))(input_layer)\n",
    "cnn_out = TimeDistributed(MaxPooling2D(pool_size=(3, 3)))(cnn_out)\n",
    "cnn_out = TimeDistributed(Conv2D(filters=64, kernel_size=(4, 4), activation='relu'))(cnn_out)\n",
    "cnn_out = TimeDistributed(MaxPooling2D(pool_size=(3, 3)))(cnn_out)\n",
    "cnn_out = TimeDistributed(Conv2D(filters=128, kernel_size=(4, 4), activation='relu'))(cnn_out)\n",
    "cnn_out = TimeDistributed(MaxPooling2D(pool_size=(3, 3)))(cnn_out)\n",
    "cnn_out = TimeDistributed(Flatten())(cnn_out)  # Flatten each frame's feature map\n",
    "\n",
    "# If you want to aggregate features across time, you can use Flatten directly or a pooling layer across the time dimension\n",
    "# Flatten to prepare for Dense layers (e.g., merging time and feature maps into one long vector)\n",
    "cnn_out = Flatten()(cnn_out)\n",
    "\n",
    "# Scalar inputs for the best track data at present (shape=(6,))\n",
    "best_track_input = Input(shape=(6,))\n",
    "\n",
    "# Concatenate the scalar values with the output of the CNN\n",
    "combined_inputs = Concatenate(axis=-1)([cnn_out, best_track_input])\n",
    "\n",
    "# Dense layers for final prediction\n",
    "dense_out = Dense(units=100, activation='relu')(combined_inputs)\n",
    "output = Dense(units=4)(dense_out)  # Predicting 4 values for the cyclone's next location\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[input_layer, best_track_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_Track=pd.read_csv(\"BestTrack_Trimmed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# firstFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_Track['ISO_TIME']=pd.to_datetime(best_Track['ISO_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime=pd.to_datetime(gribName[:8],format='%Y%m%d')\n",
    "endtime=starttime+pd.Timedelta(days=numDays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fBesttrack=best_Track[(best_Track['ISO_TIME']>=starttime)&(best_Track['ISO_TIME']<=endtime)&(best_Track['LAT']>=0.0)&(best_Track['LAT']<=90.0)&(best_Track['LON']>=-170.0)&(best_Track['LON']<=-80.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fBesttrack=fBesttrack[fBesttrack['NAME']==cyc_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = fBesttrack[['USA_WIND', 'USA_PRES', 'LAT', 'LON','ISO_TIME']]\n",
    "selected_columns=selected_columns[selected_columns['ISO_TIME'].dt.hour%3==0]\n",
    "def normalize(value, min_val, max_val):\n",
    "    return (value - min_val) / (max_val - min_val)\n",
    "\n",
    "# Define the ranges\n",
    "lat_min, lat_max = 0, 40\n",
    "lon_min, lon_max = -170, -80\n",
    "wind_min, wind_max = 20, 200\n",
    "pressure_min, pressure_max = 860, 1020\n",
    "\n",
    "# Normalize the data\n",
    "selected_columns['LAT'] = normalize(selected_columns['LAT'], lat_min, lat_max)\n",
    "selected_columns['LON'] = normalize(selected_columns['LON'], lon_min, lon_max)\n",
    "selected_columns['USA_WIND'] = normalize(selected_columns['USA_WIND'].astype(float), wind_min, wind_max)\n",
    "selected_columns['USA_PRES'] = normalize(selected_columns['USA_PRES'].astype(float), pressure_min, pressure_max)\n",
    "selected_columns['ISO_DATE'] = selected_columns['ISO_TIME'].dt.dayofyear/365\n",
    "selected_columns['ISO_TIME'] = selected_columns['ISO_TIME'].dt.hour/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "starttimeb=fBesttrack['ISO_TIME'].iloc[0]\n",
    "endtimeb=fBesttrack['ISO_TIME'].iloc[-1]\n",
    "assert (endtimeb-starttimeb)/datetime.timedelta(seconds=3*3600) == len(selected_columns)-1 #same frame count\n",
    "\n",
    "besttrack_ndarr=selected_columns.to_numpy().astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('ibtracs.EP.list.v04r00.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trim=df[['SID','NAME','SEASON','NUMBER','NATURE','ISO_TIME','LAT','LON','WMO_WIND','WMO_PRES','USA_RECORD','USA_STATUS','USA_WIND','USA_PRES','USA_GUST']][1:]\n",
    "# df_trim['SEASON']=pd.to_numeric(df_trim['SEASON'])\n",
    "# df_trim=df_trim[df_trim['SEASON']>=2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_trim.to_csv('BestTrack_Trimmed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "d"
    ]
   },
   "outputs": [],
   "source": [
    "#import cdsapi\n",
    "#c = cdsapi.Client()\n",
    "#c.retrieve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1618255376815796"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inTensor.size*inTensor.itemsize)/(1024*1024*1024)#in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "startFrameinA=int((starttimeb-starttime)/datetime.timedelta(hours=1))\n",
    "endFrameinA=int((endtimeb-starttime)/datetime.timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def Bdx_to_Adx(bdx, startFrameinA):\n",
    "    \"\"\" Converts an index in best track data to its corresponding index in inTensor. \"\"\"\n",
    "    return bdx * 3 + startFrameinA\n",
    "\n",
    "def data_generator(inTensor, besttrack_ndarr, startFrameinA, endFrameinA, scope=12, forecast_horizon=6):\n",
    "    \"\"\"\n",
    "    Generator function to yield individual training data samples and labels.\n",
    "    \"\"\"\n",
    "    # Calculate the number of samples available for generation\n",
    "    num_samples = len(besttrack_ndarr) - forecast_horizon // 3\n",
    "    indices = np.arange(0, num_samples)\n",
    "    np.random.shuffle(indices)  # Shuffle indices for random sampling\n",
    "    \n",
    "    while True:  # Loop indefinitely for continuous generation\n",
    "        for idx in indices:\n",
    "            # Calculate starting frame index for inTensor based on best track data index\n",
    "            start_idx = Bdx_to_Adx(idx, startFrameinA) - scope\n",
    "            end_idx = Bdx_to_Adx(idx, startFrameinA) + 1\n",
    "            \n",
    "            # Generate input frames from inTensor\n",
    "            cnn_input = np.stack(inTensor[start_idx:end_idx])\n",
    "            \n",
    "            # Get corresponding best track input\n",
    "            track_input = besttrack_ndarr[idx]\n",
    "            \n",
    "            # Prepare target output based on forecast horizon\n",
    "            target = besttrack_ndarr[idx + forecast_horizon // 3][:4]\n",
    "            \n",
    "            if cnn_input is None or track_input is None or target is None:\n",
    "                print(\"None found in generator output!\")\n",
    "                continue  # or handle it differently as needed\n",
    "            \n",
    "            # print(\"Yielding data:\", cnn_input.shape, track_input.shape, target.shape)\n",
    "            # Yield the individual data sample and target\n",
    "            yield ((cnn_input, track_input), target)\n",
    "\n",
    "# Define output signatures to specify the shapes and types of outputs expected\n",
    "output_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(13, *gridShape, 60), dtype=tf.float32),  # CNN input shape\n",
    "        tf.TensorSpec(shape=(6,), dtype=tf.float32)  # Best track input shape\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(4,), dtype=tf.float32)  # Output target shape\n",
    ")\n",
    "\n",
    "# Create a TensorFlow dataset from the generator, specifying batch size within the dataset pipeline\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator=lambda: data_generator(inTensor, besttrack_ndarr, startFrameinA, endFrameinA),\n",
    "    output_signature=output_signature\n",
    ").batch(1)  # Define batch size here\n",
    "\n",
    "# Example usage: Ready for model training\n",
    "# model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/4\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 1s/step - loss: 1475.3142"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.fit(x=dataset, steps_per_epoch=4, epochs=5)\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ref_minmax(data):\n",
    "    assert data.shape[2]==12 and data.shape[1]==5 #60 channels\n",
    "    minarr=np.zeros(12)\n",
    "    maxarr=np.zeros(12)\n",
    "    for channel in range(data.shape[2]):  # Assuming the third index is for channels\n",
    "        # Extract the channel data\n",
    "        channel_data = data[:, :, channel, :, :]\n",
    "        \n",
    "        # Compute the min and max\n",
    "        minarr[channel] = np.min(channel_data)\n",
    "        maxarr[channel] = np.max(channel_data)\n",
    "    return minarr,maxarr\n",
    "\n",
    "def normalize_channels(data,minarr,maxarr):\n",
    "    # data.shape should be (numDays*framesPerDay, numIsoLvls, numChannels, height, width)\n",
    "    assert data.shape[2]==12 and data.shape[1]==5 #60 channels\n",
    "    # Initialize a new array to hold the normalized data\n",
    "    normalized_data = np.zeros_like(data, dtype=np.float32)\n",
    "    \n",
    "    # Iterate over each channel to normalize\n",
    "    for channel in range(data.shape[2]):  # Assuming the third index is for channels\n",
    "        # Extract the channel data\n",
    "        channel_data = data[:, :, channel, :, :]\n",
    "        \n",
    "        # OBTAIN the min and max\n",
    "        min_val = minarr[channel]\n",
    "        max_val = maxarr[channel]\n",
    "        \n",
    "        # Avoid division by zero in case all values in the channel are the same\n",
    "        if max_val > min_val:\n",
    "            # Normalize the channel\n",
    "            normalized_data[:, :, channel, :, :] = (channel_data - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            # If max equals min, set the normalized data to zero or handle appropriately\n",
    "            normalized_data[:, :, channel, :, :] = 0  # Or set it to a neutral value like 0.5 if more appropriate\n",
    "    return normalized_data\n",
    "\n",
    "def Bdx_to_Adx(bdx, startFrameinA):\n",
    "    \"\"\" Converts an index in best track data to its corresponding index in inTensor. \"\"\"\n",
    "    return bdx * 3 + startFrameinA\n",
    "\n",
    "def data_generator(inTensor, besttrack_ndarr, startFrameinA, endFrameinA, scope=12, forecast_horizon=6):\n",
    "    \"\"\"\n",
    "    Generator function to yield individual training data samples and labels.\n",
    "    \"\"\"\n",
    "    # Calculate the number of samples available for generation\n",
    "    num_samples = len(besttrack_ndarr) - forecast_horizon // 3\n",
    "    indices = np.arange(0, num_samples)\n",
    "    np.random.shuffle(indices)  # Shuffle indices for random sampling\n",
    "    \n",
    "    while True:  # Loop indefinitely for continuous generation\n",
    "        for idx in indices:\n",
    "            # Calculate starting frame index for inTensor based on best track data index\n",
    "            start_idx = Bdx_to_Adx(idx, startFrameinA) - scope\n",
    "            end_idx = Bdx_to_Adx(idx, startFrameinA) + 1\n",
    "            \n",
    "            # Generate input frames from inTensor\n",
    "            cnn_input = np.stack(inTensor[start_idx:end_idx])\n",
    "            \n",
    "            # Get corresponding best track input\n",
    "            track_input = besttrack_ndarr[idx]\n",
    "            \n",
    "            # Prepare target output based on forecast horizon\n",
    "            target = besttrack_ndarr[idx + forecast_horizon // 3][:4]\n",
    "            \n",
    "            if cnn_input is None or track_input is None or target is None:\n",
    "                print(\"None found in generator output!\")\n",
    "                continue  # or handle it differently as needed\n",
    "            \n",
    "            # print(\"Yielding data:\", cnn_input.shape, track_input.shape, target.shape)\n",
    "            # Yield the individual data sample and target\n",
    "            yield ((cnn_input, track_input), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 5, 12, 221, 381)\n"
     ]
    }
   ],
   "source": [
    "import pygrib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "gribName=\"20151018_PATRICIA_1024\"\n",
    "cyc_name=str.upper(\"PATRICIA\")\n",
    "grbs = pygrib.open('.\\\\Data\\\\'+gribName+'.grib')\n",
    "grb=grbs[1:]\n",
    "grb.append(grbs[len(grb)+1])\n",
    "del grbs\n",
    "\n",
    "numChannels=12\n",
    "gridShape=(221,381)\n",
    "framesPerDay=24\n",
    "isoLvls=[500,700,850,925,1000]\n",
    "numIsoLvls=len(isoLvls)\n",
    "numFrames=len(grb)\n",
    "numDays=numFrames//(numChannels*framesPerDay*numIsoLvls)\n",
    "#INDEX=Frame*5*12+IsoLvl*12+Channel (tot=12*5*24*numDays\n",
    "assert numFrames%(numChannels*framesPerDay*numIsoLvls)==0 #Having integer number of days\n",
    "assert grb[1].values.shape==gridShape #Check grid shape matches\n",
    "\n",
    "arr=np.zeros(shape=(numDays*framesPerDay,numIsoLvls,numChannels,*gridShape),dtype=\"float32\")\n",
    "print(arr.shape)\n",
    "\n",
    "def parseIndices(i):\n",
    "    timeLoc=i//(numChannels*numIsoLvls)\n",
    "    isoLoc=(i-timeLoc*(numChannels*numIsoLvls))//numChannels\n",
    "    channelLoc=i-timeLoc*(numChannels*numIsoLvls)-isoLoc*numChannels\n",
    "    return timeLoc,isoLoc,channelLoc\n",
    "\n",
    "#INDEX=Frame*60+IsoLvl*12+Channel (tot=12*5*24*numDays)\n",
    "for i,v in enumerate(grb):\n",
    "    arr[*parseIndices(i)] = v.values.astype(\"float32\")\n",
    "del grb\n",
    "\n",
    "minref,maxref=find_ref_minmax(arr)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "output_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(13, *gridShape, 60), dtype=tf.float32),  # CNN input shape\n",
    "        tf.TensorSpec(shape=(6,), dtype=tf.float32)  # Best track input shape\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(4,), dtype=tf.float32)  # Output target shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">221</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">381</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">219</span>,   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">17,312</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">379</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_2  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">123</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_3  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_4  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_5  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_6  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9216</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119808</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119814</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">11,981,500</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">404</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m221\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m381\u001b[0m, \u001b[38;5;34m60\u001b[0m)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m219\u001b[0m,   │     \u001b[38;5;34m17,312\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m379\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m73\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_2  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m70\u001b[0m,    │     \u001b[38;5;34m32,832\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m123\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_3  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m23\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_4  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m20\u001b[0m,    │    \u001b[38;5;34m131,200\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_5  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m12\u001b[0m, │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_6  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m9216\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m119808\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ time_distributed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m119814\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │ \u001b[38;5;34m11,981,500\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │        \u001b[38;5;34m404\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,163,248</span> (46.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,163,248\u001b[0m (46.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,163,248</span> (46.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,163,248\u001b[0m (46.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Input shape considering each frame is now part of a sequence\n",
    "input_layer = Input(shape=(13, *gridShape, 60))  # Adjust the gridShape and channels as necessary\n",
    "\n",
    "# Apply CNN to each frame independently using TimeDistributed\n",
    "cnn_out = TimeDistributed(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))(input_layer)\n",
    "cnn_out = TimeDistributed(MaxPooling2D(pool_size=(3, 3)))(cnn_out)\n",
    "cnn_out = TimeDistributed(Conv2D(filters=64, kernel_size=(4, 4), activation='relu'))(cnn_out)\n",
    "cnn_out = TimeDistributed(MaxPooling2D(pool_size=(3, 3)))(cnn_out)\n",
    "cnn_out = TimeDistributed(Conv2D(filters=128, kernel_size=(4, 4), activation='relu'))(cnn_out)\n",
    "cnn_out = TimeDistributed(MaxPooling2D(pool_size=(3, 3)))(cnn_out)\n",
    "cnn_out = TimeDistributed(Flatten())(cnn_out)  # Flatten each frame's feature map\n",
    "\n",
    "# If you want to aggregate features across time, you can use Flatten directly or a pooling layer across the time dimension\n",
    "# Flatten to prepare for Dense layers (e.g., merging time and feature maps into one long vector)\n",
    "cnn_out = Flatten()(cnn_out)\n",
    "\n",
    "# Scalar inputs for the best track data at present (shape=(6,))\n",
    "best_track_input = Input(shape=(6,))\n",
    "\n",
    "# Concatenate the scalar values with the output of the CNN\n",
    "combined_inputs = Concatenate(axis=-1)([cnn_out, best_track_input])\n",
    "\n",
    "# Dense layers for final prediction\n",
    "dense_out = Dense(units=100, activation='relu')(combined_inputs)\n",
    "output = Dense(units=4)(dense_out)  # Predicting 4 values for the cyclone's next location\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[input_layer, best_track_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null WP values removed\n",
      "      USA_WIND USA_PRES      LAT       LON            ISO_TIME    NAME\n",
      "12522       15           12.3000  -98.2000 2023-06-25 06:00:00  ADRIAN\n",
      "12523       15           12.4125  -98.2900 2023-06-25 09:00:00  ADRIAN\n",
      "12524       15           12.5000  -98.4000 2023-06-25 12:00:00  ADRIAN\n",
      "12525       15           12.5500  -98.5425 2023-06-25 15:00:00  ADRIAN\n",
      "12526       15           12.6000  -98.7000 2023-06-25 18:00:00  ADRIAN\n",
      "...        ...      ...      ...       ...                 ...     ...\n",
      "13740       25           10.5500 -114.5530 2023-11-17 21:00:00   RAMON\n",
      "13741       25           10.5000 -114.7000 2023-11-18 00:00:00   RAMON\n",
      "13742       25           10.4500 -114.8420 2023-11-18 03:00:00   RAMON\n",
      "13743       25           10.4000 -115.0000 2023-11-18 06:00:00   RAMON\n",
      "13744       25           10.3500 -115.1930 2023-11-18 09:00:00   RAMON\n",
      "\n",
      "[64 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def file_to_arr(fname):\n",
    "    global minref,maxref\n",
    "    global numFrames,numDays\n",
    "\n",
    "    # cyc_name=str.upper(fname.split('_')[1])\n",
    "    grbs = pygrib.open('.\\\\Data\\\\'+fname+'.grib')\n",
    "\n",
    "    grb=grbs[1:]\n",
    "    grb.append(grbs[len(grb)+1])\n",
    "\n",
    "    firstDay=grb[0]['day']\n",
    "    assert fname.split('_')[0][-2:]==str(firstDay)\n",
    "    \n",
    "    numFrames=len(grb)\n",
    "    numDays=numFrames//(numChannels*framesPerDay*numIsoLvls)\n",
    "    #INDEX=Frame*5*12+IsoLvl*12+Channel (tot=12*5*24*numDays)\n",
    "    assert numFrames%(numChannels*framesPerDay*numIsoLvls)==0 #Having integer number of days\n",
    "    assert grb[1].values.shape==gridShape #Check grid shape matches\n",
    "    \n",
    "    numFrames=len(grb)\n",
    "    numDays=numFrames//(numChannels*framesPerDay*numIsoLvls)\n",
    "\n",
    "    for i,v in enumerate(grb):\n",
    "        arr[*parseIndices(i)] = v.values.astype(\"float32\")\n",
    "\n",
    "    del grb\n",
    "    del grbs\n",
    "    \n",
    "    \n",
    "    normalized_arr = normalize_channels(arr,minref,maxref)\n",
    "    transposed_arr = np.transpose(normalized_arr, (0, 3, 4, 1, 2))\n",
    "\n",
    "    # Now, we need to merge the last two dimensions (numIsoLvls and numChannels)\n",
    "    # Calculate the new dimension\n",
    "    new_last_dim = transposed_arr.shape[3] * transposed_arr.shape[4]\n",
    "\n",
    "    # Reshape to merge the last two dimensions\n",
    "    inTensor = transposed_arr.reshape(transposed_arr.shape[0], transposed_arr.shape[1], transposed_arr.shape[2], new_last_dim)\n",
    "\n",
    "    return inTensor, numDays\n",
    "\n",
    "def normalize(value, min_val, max_val):\n",
    "    return (value - min_val) / (max_val - min_val)\n",
    "\n",
    "def load_btrack(fname=\"BestTrack_Trimmed.csv\"):\n",
    "    best_Track=pd.read_csv(fname)\n",
    "    best_Track['ISO_TIME']=pd.to_datetime(best_Track['ISO_TIME'])\n",
    "    fBesttrack=best_Track[(best_Track['ISO_TIME']>=pd.to_datetime('20150101',format='%Y%m%d'))&(best_Track['ISO_TIME']<=pd.to_datetime('20240101',format='%Y%m%d'))&(best_Track['LAT']>=0.0)&(best_Track['LAT']<=90.0)&(best_Track['LON']>=-170.0)&(best_Track['LON']<=-80.0)]\n",
    "    selected_columns = fBesttrack[['USA_WIND', 'USA_PRES', 'LAT', 'LON','ISO_TIME','NAME']]\n",
    "    selected_columns=selected_columns[selected_columns['ISO_TIME'].dt.hour%3==0]\n",
    "\n",
    "    badrows=selected_columns[(selected_columns['USA_WIND']==' ') | (selected_columns['USA_PRES']==' ')]\n",
    "    print(\"Null WP values removed\")  \n",
    "    print(badrows)\n",
    "    selected_columns=selected_columns[(selected_columns['USA_WIND']!=' ')&(selected_columns['USA_PRES']!=' ')]\n",
    "\n",
    "    # Define the ranges\n",
    "    lat_min, lat_max = 0, 40\n",
    "    lon_min, lon_max = -170, -80\n",
    "    wind_min, wind_max = 20, 200\n",
    "    pressure_min, pressure_max = 860, 1020\n",
    "    selected_columns['LAT'] = normalize(selected_columns['LAT'], lat_min, lat_max)\n",
    "    selected_columns['LON'] = normalize(selected_columns['LON'], lon_min, lon_max)\n",
    "    selected_columns['USA_WIND'] = normalize(selected_columns['USA_WIND'].astype(float), wind_min, wind_max)\n",
    "    selected_columns['USA_PRES'] = normalize(selected_columns['USA_PRES'].astype(float), pressure_min, pressure_max)\n",
    "    selected_columns['ISO_DATE'] = selected_columns['ISO_TIME'].dt.dayofyear/365\n",
    "    selected_columns['ISO_HOUR'] = selected_columns['ISO_TIME'].dt.hour/24\n",
    "    return selected_columns\n",
    "\n",
    "btrack=load_btrack()\n",
    "\n",
    "def load_grib(*fname):\n",
    "    assert len(fname)==1 or len(fname)==2\n",
    "    cyc_name=str.upper(fname[0].split('_')[1])\n",
    "    inTensor=None\n",
    "    numdays=0\n",
    "\n",
    "    if(len(fname)==2):\n",
    "        intensorA,numDays=file_to_arr(fname[0])\n",
    "        intensorB,day_concat=file_to_arr(fname[1])\n",
    "        inTensor = np.concatenate((intensorA,intensorB),axis=0)\n",
    "        numDays+=day_concat\n",
    "    else: \n",
    "        inTensor,numDays=file_to_arr(fname[0])\n",
    "    \n",
    "    global btrack\n",
    "    starttime=pd.to_datetime(fname[0][:8],format='%Y%m%d')\n",
    "    endtime=starttime+pd.Timedelta(days=numDays)\n",
    "    print(btrack.head(5))\n",
    "    best_Track=btrack[(btrack['ISO_TIME']>=starttime)&(btrack['ISO_TIME']<=endtime)&(btrack['NAME']==cyc_name)]\n",
    "\n",
    "    starttimeb=best_Track['ISO_TIME'].iloc[0]\n",
    "    endtimeb=best_Track['ISO_TIME'].iloc[-1]\n",
    "    startFrameinA=int((starttimeb-starttime)/datetime.timedelta(hours=1))\n",
    "    endFrameinA=int((endtimeb-starttime)/datetime.timedelta(hours=1))\n",
    "\n",
    "    assert (endtimeb-starttimeb)/datetime.timedelta(seconds=3*3600) == len(best_Track)-1 #same frame count\n",
    "    besttrack_ndarr=best_Track[['USA_WIND', 'USA_PRES', 'LAT', 'LON','ISO_DATE','ISO_HOUR']].to_numpy().astype(\"float32\")\n",
    "\n",
    "    global dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator=lambda: data_generator(inTensor, besttrack_ndarr, startFrameinA, endFrameinA),\n",
    "        output_signature=output_signature\n",
    "    ).batch(3)  # Define batch size here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     USA_WIND  USA_PRES       LAT       LON            ISO_TIME    NAME  \\\n",
      "143  0.055556   0.90625  0.270000  0.667778 2015-05-28 06:00:00  ANDRES   \n",
      "144  0.083333   0.89375  0.273690  0.661756 2015-05-28 09:00:00  ANDRES   \n",
      "145  0.111111   0.88750  0.277500  0.655556 2015-05-28 12:00:00  ANDRES   \n",
      "146  0.138889   0.87500  0.281438  0.649200 2015-05-28 15:00:00  ANDRES   \n",
      "147  0.166667   0.86250  0.285000  0.643333 2015-05-28 18:00:00  ANDRES   \n",
      "\n",
      "     ISO_DATE  ISO_HOUR  \n",
      "143  0.405479     0.250  \n",
      "144  0.405479     0.375  \n",
      "145  0.405479     0.500  \n",
      "146  0.405479     0.625  \n",
      "147  0.405479     0.750  \n"
     ]
    }
   ],
   "source": [
    "load_grib(\"20151018_PATRICIA_1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 4s/step - loss: 28.8410\n",
      "Epoch 2/4\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - loss: 0.0780\n",
      "Epoch 3/4\n",
      "\u001b[1m 3/10\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 4s/step - loss: 0.0674"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nMemoryError: Unable to allocate 251. MiB for an array with shape (13, 221, 381, 60) and data type float32\nTraceback (most recent call last):\n\n  File \"c:\\Users\\KelM\\anaconda3\\envs\\ecmwf2\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\KelM\\anaconda3\\envs\\ecmwf2\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\KelM\\anaconda3\\envs\\ecmwf2\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\KelM\\AppData\\Local\\Temp\\ipykernel_4376\\3364562540.py\", line 58, in data_generator\n    cnn_input = np.stack(inTensor[start_idx:end_idx])\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\KelM\\anaconda3\\envs\\ecmwf2\\Lib\\site-packages\\numpy\\core\\shape_base.py\", line 456, in stack\n    return _nx.concatenate(expanded_arrays, axis=axis, out=out,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 251. MiB for an array with shape (13, 221, 381, 60) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_one_step_on_iterator_5348]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KelM\\anaconda3\\envs\\ecmwf2\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\KelM\\anaconda3\\envs\\ecmwf2\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nMemoryError: Unable to allocate 251. MiB for an array with shape (13, 221, 381, 60) and data type float32\nTraceback (most recent call last):\n\n  File \"c:\\Users\\KelM\\anaconda3\\envs\\ecmwf2\\Lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"c:\\Users\\KelM\\anaconda3\\envs\\ecmwf2\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\KelM\\anaconda3\\envs\\ecmwf2\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\KelM\\AppData\\Local\\Temp\\ipykernel_4376\\3364562540.py\", line 58, in data_generator\n    cnn_input = np.stack(inTensor[start_idx:end_idx])\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\KelM\\anaconda3\\envs\\ecmwf2\\Lib\\site-packages\\numpy\\core\\shape_base.py\", line 456, in stack\n    return _nx.concatenate(expanded_arrays, axis=axis, out=out,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 251. MiB for an array with shape (13, 221, 381, 60) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_one_step_on_iterator_5348]"
     ]
    }
   ],
   "source": [
    "model.fit(x=dataset, steps_per_epoch=10, epochs=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
